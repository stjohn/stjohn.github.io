<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  	<title>Challenges, DuckieTown@Hunter, January 2020</title>
    <link type="text/css" rel="stylesheet" href="../../lehman.css" />
  <style type="text/css">
  	li {
  		  padding-bottom:	10px ;
  }
  </style>
</head>
<body>

<div>
<div style =  "float:right">
<img style =  "height:100" src="duckie.png"><br>
<!--<p style = "float:right"> <em><small>(Image: everythingbranded.com)</small></em>-->
</div>


<h1>Challenges<h1>
<h2>DuckieTown Workshop<br>
Hunter College<br>
January 2020</h2>
</div>


<div>
The challenges below are part of the DuckieTown@Hunter workshop in January 2020.
The four-day, hands-on workshop introduces challenges in autonomous navigation with Duckietown. <a href="http://duckietown.org">Duckietown</a> is a colorful, realistic, internationally used platform that makes research issues in autonomous driving accessible. 



<ul>
	<li> Monday:  Introduction to Robotics
		<ul>
			<li> <a href="#monAM">Monday Morning:  Getting Started: Calibrations & Test Runs</a>
			<li> <a href="#monPM">Monday Afternoon:  Lane Following Demos</a>
		</ul>
	<li> Tuesday:  Navigation
		<ul>
			<li> <a href="#tueAM">Tuesday Morning:  First Program:  Motor Control</a>
			<li> <a href="#tuePM">Tuesday Afternoon:  Navigation &  the A* Search Algorithm  </a>
		</ul>	
	<li> Wednesday: Vision
		<ul>
			<li> <a href="#wedAM">Wednesday Morning:  Color Detection</a>
			<li> <a href="#wedPM">Wednesday Afternoon:  Analyziing Images  & DuckieTown with Traffic Control</a>
		</ul>		
	<li> Thursday: Machine Learning & Ethics
		<ul>
			<li> <a href="#thuAM">Thursday Morning:  Edge Detection</a>
			<!--<li> <a href="#tuePM">Tuesday Afternoon:  Navigation &  A* Search Algorithm  </a>-->
		</ul>		
</ul>

<p><i>This workshop was made possible by a <a href="http://services.google.com/fh/files/misc/2019_explorecsr_grantees.pdf">Google Award for Undergraduate Computer Science Research Focused Workshops for Women</a>  and the <a href="http://www.hunter.cuny.edu/provost">Office of the Provost</a> at Hunter College.</i>

</div>

<div>
<a id="monAM"></a>
<h3>Monday Morning</h3>
<hr>
<h2>Getting Started: Calibrations & Test Runs</h2>
<hr>

<p><b>Goal:</b> To drive a calibrated robot through both towns, using the joystick program.
<p><b>Equipment needed:</b>  Department linux laptop and duckieBot.

<hr>


<h3>Setting Up DuckieBots</h3>

<p>When you get a duckieBot, 
<ul>
	<li> Gently plug in the two battery cables.
	<li> If the lights don't automatically already turns on, double click the side button.
	<li> Remember to take the lens cap off and put in box.	
</ul>	
It will take a few minutes for the duckieBot to boot up, so, in the meantime, we will start up the dashboard.	

<h3>Setting Up Your Laptop & DuckieTown Account</h3>

<p>
We are using the linux laptops from the 1001E laboratory (the CSci 127 lab).  Cart 1 laptops have the duckieTown software already installed.  The laptops run Ubuntu Linux operating system.   When you open the laptop, choose the "Computer Science Guest" account (the password is on the whiteboard).  On the left hand side is a bar of icons, including icons for a browser (for accessing webpages) and the terminal window (for writing commands and launching programs).  

<p> For the morning session, we are following <a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/index.html">Duckiebot operation manual</a>.
We have the software loaded, so, we're skipping ahead to the Duckiebot assembly and setup.
<p>Go to the documentation  page  for creating a  duckieTown account:

<blockquote>
<a href="
https://docs.duckietown.org/DT19/opmanual_duckiebot/out/dt_account.html">
Duckie Op Manual C-2: Duckietown account</a>
</tt>
</blockquote>
Our regional community is New York.  Make sure to include your institution as Hunter College.   
Having an account allows you to create tokens to use the dashboard (next section).  You can copy the token by highlighting it and then clicking CTRL-Shift-C (and pasting by CTRL-Shift-V).

<p>Much of our work will done via the terminal window.  Click the icon on the left for the terminal (looks like a computer).  It will launch a terminal window in which you can type commands.

<p>
To set up your token, type at the terminal:

	<pre style = "background-color:lavender;width:400px">
$ dts tok set</pre>
	(The <tt>$</tt> represents the prompt at the command line-- no need to type it.)


<p>You can check to see that authentication worked, by typing at the terminal:
	<pre style = "background-color:lavender;width:400px">
$ dts challenges info</pre>


<p>
Make sure you are on the wifi, <tt>duckieworkshop</tt>; the password is on the  whiteboard.  The duckiebots are connected to that wifi.

<p>Check to see the robot is turned on and on the wifi:

<p style = "background-color:lavender;width:400px">
<tt>
$ ping <span style="color:indigo;background-color:white">DUCKIEBOT_NAME</span>.local
</tt>
</p>
where <span style="color:indigo">DUCKIEBOT_NAME</span> is <tt>hunterbotXX</tt> with XX as the number of your bot.   To stop the messages, type CTRL-C.

<p>
To launch the dashboard, open up Chrome browser window (Firefox will work but flicker a bit):
<ul>
	<li> Click on + key to open a new tab (the first chrome window launched defaults to private priviledges that doesn't work well well the dashboard; subsequent windows/tabs are fine).
	<li> At the navigation bar, type:  
	<blockquote>
	<tt><span style="color:indigo">DUCKIEBOT_NAME</span>.local </tt>
	</blockquote>
	where <span style="color:indigo">DUCKIEBOT_NAME</span> is the name of your bot.
	<li> You will be prompted for your token; cut-and-paste it into the text-area.
	<li> Once the dashboard launches, click on the <tt>Mission Control</tt> tab.   Open the default mission.
</ul>
You should see several graphs and a region for the camera output-- no worries if nothing shows up in that region, we're going to work on that later.  


<h3>Driving with Joystick</h3>

<p>We can run the duckieBot via the joystick demo.  It's a nice example of using the duckieTown shell (<tt>dts</tt>) and also used as the control for other demos, so, good to try:

<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/rc_control.html">
Duckie Op Manual C-7: Making your Duckiebot move</a>
</blockquote>

<p>
Now that everything is calibrated, use the joystick to make a complete lap of both tracks.  Be careful not to run into other bots.

<p>
Let's test that we can move the duckieBot through the dashboard:
<ul>
	<li> Go to the dashboard (inside the Chrome window), and in the upper right hand corner, flip the switch "Take Over".
	<li> Using the arrow keys (lower right hand corner on your keyboard), you can move your duckieBot forward (up-arrow), backwards (down-arrow), left (left-arrow), and right (right-arrow).
	<li> Try moving your duckieBot in a square.
	<li> Before going on to the next step, make sure to flip back the switch "Take Over".
</ul>

<p>
When you're done with your robot, gently disconnect the battery cables so that the batteries last longer between charges.








<h3>Calibrating Cameras</h3>

<p>
To see what your duckiebot sees, follow the directions in:
<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/read_camera_data.html">
Duckie Op Manual C-8: Duckie Op Manual C-8: See what your DuckieBot sees</a>
</blockquote>
This will launch an additional window that has a camera input, filtered in different ways.
<p>

<p>
Now, we can calibrate the camera.  To do so, you will need a calibration sheet (checkerboard patterned laminated paper).  
<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/camera_calib.html">
Duckie Op Manual C-9:  Camera calibration and validation</a>
</blockquote>

Notes on how to speed up calibration of cameras:
<ul>
	<li> Use a clipboard to hold the calibration sheet flat.
	<li> Put duckiebot at the edge of the table to get the "low" calibration points and makes it easier to get the far left and right.  
	<li> It won't turn green until all regions have a reading-- Move the calibration as high as possible (and still have highlighted colors on grid), as low, to far left and to far right as possible.   Also do the 4 "corners" of where it can see.  
	<li> Repeat this cycle 2 or 3 times, until calibration button lights up.
	<li> Hold the calibration sheet at each point for a count of 3 (too fast and it doesn't read it).
</ul>




<h3>Calibrating Wheels</h3>

Now, we can calibrate the wheels.  To do so, you will a long straight track (we have two marked in masking tape).  Each tile is about 2 feet or 60 centimeters long. 
<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/wheel_calibration.html">
Duckie Op Manual C-10:  Wheel calibration</a>
</blockquote>



<hr>
<h3>Extra Challenges</h3>

<ul>
	<li><b>Dashboard Measures</b>  What do the monitors on the dashboard measure?  With the joystick controls, drive your duckie.  Which monitor measure velocity?  What happens when you drive "backwards"?
	
	<li><b>What does the camera see?</b>  Turn your back to the duckieBot, and drive only using the camera feed (no looking at track).  <!--the Launch the video debug feed.  Taking a photo (can this be done just from dts, or do we need to load a program?)-->
	
	<li><b>Portainers:</b>  The code for the duckieBots is loaded in "containers."  You can monitor which containers are on your duckiebot, by opening a new browser window and typing into a the navigation bar:
	<blockquote>
	<tt><span style="color:indigo">DUCKIEBOT_NAME</span>.local:9000</tt>
	</blockquote>
	How many containers on your duckieBot?  What is their status?  We will be adding more containers in later sessions.

</ul>
 
</div>

<div>
<a id="monPM"></a>
<h3>Monday Afternoon</h3>
<hr>
<h2>Lane Following Demos</h2>
<hr>
<p><b>Goal:</b> Run and understand the structure of the lane following demo.

<p><b>Equipment needed:</b>  Department linux laptop and duckieBot.  Paper and pencil for analyzing code structure.
<hr>

<h3>Running Demo Programs</h3>

<p>DuckieTown comes with several pre-built demostration programs.  We saw the joystick demo this morning.  Several other programs use it as their "front end" and operation is done via its interface.  One such demo is the lane following, where the ducks move forward, adjusting their direction to keep the white line on their right and the yellow dotted line in the middle.

<p style = "background-color:lavender;width:800px">
<tt>
$ dts duckiebot demo --demo_name lane_following --duckiebot_name <span style="color:indigo;background-color:white">DUCKIEBOT_NAME</span> --package_name duckietown_demos
</tt>
</p>

<p>You can run the lane following demo, by clicking on the joystick controls and typing 'a' for autonomous navigation, and 's' for stop.  You can toggle between driving manually with the arrow keys and allowing the lane following code to drive the duckieBot.

<p>The lane following demo does best with a continuous lanes that does not have breaks for intersections, so, try driving first on the small town and then try on the larger town. 

<p>
Next, let's look at the line filtering (as the duckieBot sees it) and tune the parameters to keep the duckies from cutting corners in the larger town.  Start at Step 2 of:
<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/demo_lane_following.html">
Duckie Op Manual E-4: Lane Following</a>
</blockquote>
As it runs, it generates multiple image views that highlight the lines, signs, and obstacles detected as it moves (you can toggle through these on  <tt>rqt_image_view</tt>).  How does the bot read a sign?  Brainstorm on what ROS nodes are loaded and sketch the diagram of how the nodes interact.

<hr>
<h3>Structure of the Lane Following Program</h3>

<p> Now that you have run the lane following program, let's examine how it works.  Here is an overview of the ROS code from <a href="https://idsc.ethz.ch/education/lectures/duckietown.html">Autonomous Mobility on Demand: From Car to Fleet</a> course at ETH Zurich:

<img src = "2019_ETH_autonomyCourse_LF_ROS_slide.png" width=100%>

<p>
In the diagram, 
<ul>
	<li> Ovals represent nodes-- these include the sensors (camera) and the actuators (motors) as well as the executables (parts of the program that "does things"). Nodes can subscribe to topics, process the information there, and publish the results to a topic. 
	<li> Rectangles represent "topics" which nodes can publish (i.e. write) information and subscribe (i.e. read) information
	<li> If a node publishes to a topic, there is an arrow starting at the node and pointing to the topic.
	<li> If a node subscribes to a topic, there is an arrow starting at the topic and pointing to the node.
</ul>

<p>With your team, come up with a one-line description of what each node and topic represent in diagram.  Make your best guess based on the names and your experience with the bot and feel free to ask questions!

<hr>
<h3>Extra Challenges</h3>

<p>If you have time, try the indefinite navigation demo.  In addition to lane following, it will stop at intersections and recognize signs that are placed on the town.
<blockquote>
<a href="https://docs.duckietown.org/DT19/opmanual_duckiebot/out/demo_indefinite_navigation.html">
Duckie Op Manual E-5: Indefinite Navigation</a>
</blockquote>
As it runs, it generates multiple image views that highlight the lines, signs, and obstacles detected as it moves (you can toggle through these on the <tt>rqt_image_view</tt>).  How does the bot read a sign?  Brainstorm on what ROS nodes are loaded and sketch the diagram of how the nodes interact.


</div>



<div>
<a id="tueAM"></a>
<h3>Tuesday Morning</h3>
<hr>
<h2></h2>
<h2>First Program:  Motor Control</h2>
<hr>
<p><b>Goals:</b>  Create and run a program, via the AIDO framework. 
<p><b>Equipment needed:</b>  department linux laptop and duckieBot.
<hr>

<p> The <a href="https://www.duckietown.org/research/ai-driving-olympics">AI Driving Olympics</a> (AI-DO) is an autonomous navigation competition for mobile robotics.  The competition uses the the Duckietown platform and has a series of increasingly difficult challenges, starting with lane following for a single bot to the more complex task of managinig a fleet of bots on the road.  While we won't be submitting to the competition (the last one was in December in conjunction with the NeurIPS 2019 conference in Zurich), we will use their framework.
The AI-DO framework is set up so that participants will not need to be physically present at any stage of the competition, just need to submit their source code remotely. If the submitted code does well in simulation, it is run on real duckieBots.  

<p> Our first program is a (slightly-modified) version of the <a href="https://docs.duckietown.org/DT19/AIDO/out/minimal_template.html">lane following template</a> from the AI-DO2 (using the dt19 software).  It goes straight forward, at a leisurely pace.  Work through the following steps to download the code and run the code:
<ol>
	<li> Download the <a href="slowWalk.zip">zip file</a> (right click and save to your desktop).
	<li> Double click on the zip file to "unpack" it.  It will create a folder named <tt>slowWalk</tt> on your desktop.
	<li> Open up a terminal window (left hand menu) and change directories to the new folder:
	<p style = "background-color:lavender;width:600px">
<tt>
$ cd  ~/Desktop/slowWalk
</tt>
</p>
	<li> To run the template on a duckieBot:
	<p style = "background-color:lavender;width:600px">
<tt>
$ dts duckiebot evaluate --duckiebot_name <span style="color:indigo;background-color:white">DUCKIEBOT_NAME  --duration 20</span> 
</tt> 
to run for 20 seconds (you can change the 20 to something bigger if you would like to run longer).
It will ask you to confirm the key fingerprint (type: yes) and for the duckie login's password (<tt>quackquack</tt>).   
The first time you download a program will take 3-5 minutes to load the competition framework onto the robot.  Future downloads will still ask for the  password but will be very quick.
</p>
	<li> Next, let's look at the code that the duckieBot is running.  At the  terminal window type
<p style = "background-color:lavender;width:800px">
<tt>
$ gedit & 
</tt>
</p>
	to open the built-in graphical editor and run in the background (so we can continue to use the terminal to type).
	<li> In <tt>gedit</tt>, open the file <tt>solution.py</tt> from the <tt>duckieTemplate</tt> folder.

</ol>

<p>The file, <tt>solution.py</tt>, is the Python code that does the lane following.  This particular file is very simplistic:  it ignores everything it sees and just goes forward. The two functions that we will work with are:
<ul>
	<li> <tt>on_received_observations()</tt> which reads an image whenever it becomes available, and
	<li> <tt>on_received_get_commands()</tt> which generates commands for the duckieBot.
</ul>

<p>We will start with the latter:
<p>
<img src="slowWalk.png" width=600 border=1>
<p> The function: 
	<ul> 
		<li> Starts by setting the power of the motors (<tt>pwm_left</tt> and <tt>pwm_right</tt>) to be constantly 10% power.  
		<li> It sets the commands:  the <tt>led_commands</tt> are set to gray and the <tt>pwd_commands</tt> are set to 10%.  
		<li>It then writes the commands for the LEDs and motors.
		<li> Updates the count, <tt>self.n</tt>.
	</ul>
When the camera takes a new image, this function is called.  Each time, it does exactly the same thing:  keeping two motors at 0.10.  

<p> <b>Challenge:</b>  Make your duckieBot go a  bit faster:  0.25 instead of 0.10.  Modify the <tt>solution.py</tt> and load on to your duckieBot following the same procedure as above.

<p> <b>Challenge:</b>  Make your duckieBot go in a wide circle.  Modify the <tt>solution.py</tt> and load on to your duckieBot following the same procedure as above.

<p> <b>Challenge:</b>  What does the default template do?  Try on your robot and read throught the python file, <tt>solutions.py</tt>.  What lines control it's motion?

<hr>
<h3>Extra Challenges</h3>

<p> <b>Extra Challenge:</b>  Modify your program to make a spiral (goes in increasingly large circles).

<p> <b>Extra Challenge:</b>  Modify your program to make a figure 8.  


</div>

<div>
<a id="tuePM"></a>
<h3>Tuesday Afternoon</h3>
<hr>
<h2>Navigation & the A* Search Algorithm </h2>
<hr>
<p><b>Goal:</b>  Understand the A* algorithm.
<p><b>Equipment needed:</b>  pen/pencil and paper worksheet.  For challenges: department linux laptop and duckieBot. 

<hr>

<p>
<img src="run2level6_14by14.png" height=200>
<img src="run6level6_14by14.png" width=200>


<h3>Navigation</h3>

<p>Using the A* search algorithm discussed in the slides, work through the examples on the handout.  An overview of A* is available on <a href="https://en.wikipedia.org/wiki/A*_search_algorithm">wiki</a>. 




<p> <b>Challenge:</b>   Work through any remaining challenges from the morning.

<p> <b>Challenge:</b>  The original package that our slow walk is based had the file, <a href="https://github.com/duckietown/challenge-aido_LF-template-random/blob/aido2/solution.py">solution.py</a>.  What happens if you use this file instead of the one we had in slow walk?

<hr>
<h3>Extra Challenges</h3>

<p> <b>Extra Challenge:</b>  Modify our slow walk program from this morning to make your duckie turn in place.
(Hint: how do you make your robot move backwards?)
</ul>


</div>



<div>
<a id="wedAM"></a>
<h3>Wednesday Morning</h3>
<hr>
<h2>Vision:  Color Detection</h2>
<hr>
<p><b>Goal:</b>  
<p><b>Equipment needed:</b>  department linux laptop and duckieBot, sheets of different color paper.
<hr>

<h3>Calibration</h3>

We would like to run the duckieTown city demo today (where ducks follow the traffic light, signs, and rules of the road).   This works much better with calibrated bots.
Start today by recalibrating the cameras (do both the intrinsic and extrinsic calibrations) as well as the wheels.  It will be much faster with fewer machines connected to the router (if you are not connecting to a duckieBot, use HunterNet, instead of duckieWorkshop).

<h3>Measuring Color</h3>

We have two challenges for today:  slowing down when you enter a dark tunnel and turning when the roadway changes colors.  For both, we need to measure how your
duckieBot preceives color.    

<p>To do that, you need:
<ul>
	<li> The worksheet with the grid for storing your color measurements, 
	<li> 4 different colors of paper (white plus 3 others of your choosing), and
	<li> The program in <a href="colorMeas.zip">colorMeas.zip</a>.
</ul>

<p><b>Challenge: </b> Using the data you collected, choose two colors that are easy to distinguish from the black tile.  Modify the program so that:
<ul>
	<li> The bot goes straight forward, at a leisurely pace, on the black tile.
	<li> When it sees paper of the first color, it turns left and stops.
	<li> When it sees paper of the second color, it turns right and stops.
</ul>

<p><i>Hint: you may find the thresholds approach from counting snowpack in CSci 127 <a href="https://stjohn.github.io/teaching/csci127/f19/lab4.html">Lab 4</a> useful.</i>


<h3>Extra Challenges</h3>

<p><b>Extra Challenge:</b> Modify the program so that the color averages are only for the bottom half of the image (the part dominated by the roadway/color blocks).  

<p><i>Hint:  Find the height and width (<tt>_rgb.shape[0]</tt> and (<tt>_rgb.shape[1]</tt>) and use slices before taking the average.</i>
<p>
<img src = "csBridgeIndexed.png" width="40%">&nbsp &nbsp
<img src = "csBridgeShaded.png" width="40%">

</div>

<div>
<a id="wedPM"></a>
<h3>Wedneseday Afternoon</h3>
<hr>
<h2>Vision:  Analyzing Images & DuckieTown with Traffic Control</h2>
<hr>
<p><b>Goal:</b>  Analyze camera data to turn on headlights in the dark (and turn off when light again).
<p><b>Equipment needed:</b>  department linux laptop and duckieBot.
<hr>

Our next challenge is to use camera data to inform the behavior of the duckieBot.  To be a safe duckieBot, we want it to turn on its headlights in the  dark and turn the headlights back off when it is light.

<p>
Using the programs from this morning, measure the color readings of the tunnel.  How does that differ from the regular track?  Pick a threshold value for which the values in the tunnel are less than it and those outside the tunnel are greater than that.

<p><b>Challenge: </b>  Write a program that your robot moves at a moderate speed and then slows down to a crawl when entering a tunnel.  Make sure to test and adjust your motor speeds so that your bot goes relatively straight before testing in the tunnel. 


<hr>
<h3>Extra Challenges</h3>

<p><b>Extra Challenge:</b>  Instead of barcodes on the floor (like the Amazon warehouse), lets use color regions to indicate change in directions.  Building on the program from this morning, set up a series of color coded regions, where the first color has your bot turn left, the second color has your bot turn right, and the last color has your bot stop.
</div>

<div>
<a id="thuAM"></a>
<h3>Thursday Morning</h3>
<hr>
<h2>Simplified Edge Detection</h2>
<hr>
<p><b>Goal:</b>  Make your bot follow a lane on a simplified track.
<p><b>Equipment needed:</b>  Department linux laptop and duckieBot. Measurements from yesterday.
<hr>

<p>
We have modified the small track to have the outer lane covered in white paper and the inner line the normal black tile.  The goal is to keep the bots moving forward and centered on the track:  that is, the bot should be half on the black tile and half on the white paper.  We will do this in a series of challenges:

<p><b>Challenge:</b> Modify the color measurement program to measure the fraction of white pixels (those that are above your threshold from yesterday for being on white paper) that the camera sees.  You may want to create a new instance variable (i.e. set up a variable in the <tt>init()</tt> to be used later between different functions).

<p><b>Challenge:</b> Modify your program to measure the fraction of white pixels  that the camera sees on the right side of the image.  

<p><b>Challenge:</b> Modify your program to turn left if the fraction of white pixels is significantly over two thirds, and turn right if the number is less than a third.

<p><b>Challenge:</b> Modify your program to move forward staying centered on the line.   


<hr>
<h3>Extra Challenges</h3></div>

<p><b>Extra Challenge:</b> Adjust your thresholds for determining turning  (and color) to improve performance.

<p><b>Extra Challenge:</b> Adjust your thresholds so that the white paper is always a quarter of the view (instead of half).



<!--
<div>
<a id="thuPM"></a>
<h3>Thursday Afternoon</h3>
<hr>
<h2>Machine Learning & Ethics</h2>
<hr>
<p><b>Goal:</b>
<p><b>Equipment needed:</b>  department linux laptop and duckieBot
<hr>

<hr>
<h3>Extra Challenges</h3></div>
</div>


-->


<div>
<i>Last updated 14 January 2020.</i>
</div>

</html>